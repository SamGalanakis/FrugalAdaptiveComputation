diff --git a/models/__init__.py b/models/__init__.py
index 1e2a43a..ddfdf12 100644
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -1 +1,3 @@
-from .nets import MLP
\ No newline at end of file
+from .nets import MLP
+from .frugal_rnn import FrugalRnn
+from .losses import frugal_loss
\ No newline at end of file
diff --git a/models/frugal_rnn.py b/models/frugal_rnn.py
index 6e0feee..79b2018 100644
--- a/models/frugal_rnn.py
+++ b/models/frugal_rnn.py
@@ -1,12 +1,12 @@
 import torch
 import torch.nn as nn
-from models import MLP
+from .nets import MLP
 
 
 
 
 class FrugalRnn(nn.Module):
-    def __init__(self,n_hidden,n_memory,nonlin,budget):
+    def __init__(self,n_hidden,n_memory,nonlin,hidden_dims,budget):
         super().__init__()
         self.n_hidden = n_hidden
         self.n_memory = n_memory
@@ -14,23 +14,35 @@ class FrugalRnn(nn.Module):
         self.budget = budget
 
 
-        self.iterator = MLP(n_hidden+n_memory,[64,64,64],n_hidden,nonlin)
-        self.mem_updater = MLP(n_hidden+n_memory,[64,64,64],n_memory,nonlin)
-
+        self.iterator = MLP(n_hidden+n_memory,hidden_dims,n_hidden+n_memory+2,nonlin)
+   
 
 
     def forward(self,x):
         device =x.device
         dtype = x.dtype
         batch_size = x.shape[0]
-        halted_mask = torch.zeros((batch_size,),device=device,dtype=torch.long)
+        halted_mask = torch.zeros((batch_size,),device=device,dtype=torch.long).bool()
         memory = torch.zeros((batch_size,self.n_memory),dtype=dtype,device=device)
         memory[:,0] = memory[:,0]+ self.budget
         n_iters = torch.zeros((batch_size,),dtype=dtype,device=device)
+        final_probs = torch.zeros((batch_size,),dtype=dtype,device=device)
         for index in range(self.budget):
-            if halted.all():
+            if halted_mask.all():
                 break
-
-            x = 
             
+            
+            iterator_in = torch.cat((x[~halted_mask,...],memory[~halted_mask,...]),dim=-1)
+            iterator_out = self.iterator(iterator_in)
+            
+            probs,halt_val,hidden_out,memory_out = iterator_out.split([1,1,self.n_hidden,self.n_memory],dim=-1)
+            x[~halted_mask,...] = hidden_out
+            memory[~halted_mask,...] = memory_out
+
+            halt_outs = torch.sigmoid(halt_val).round().bool().squeeze()
+            final_probs[~halted_mask] = final_probs[~halted_mask].where(~halt_outs,probs.squeeze() )
+            halted_mask[~halted_mask] = halt_outs
+
+        final_probs = final_probs.sigmoid()
+        return final_probs,n_iters
 
